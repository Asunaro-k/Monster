{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lsemi\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\Lsemi\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoProcessor, CLIPVisionModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "HUGGINGFACE_TOKEN = \"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\"\n",
    "login(HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAPTION_PATH = \"/content/Flickr8k.token.txt\"\n",
    "CAPTION_PATH = \"./Flicker/Flickr8k_text/Flickr8k.token.txt\"\n",
    "#IMAGES_FILE_PATH = \"/content/Flicker8k_Dataset\"\n",
    "IMAGES_FILE_PATH = \"./Flicker/Flickr8k_Dataset/Flicker8k_Dataset\"\n",
    "# SAVED_PATH = \"/content/saved_model/adaptor_caption.pt\"\n",
    "SAVED_PATH = \"./gemma2-9b/adaptor_caption.pt\"\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "NUM_ITERATION = 2000\n",
    "SAVE_EVERY = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "TRAIN_DATA_NUM = 7500\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available() :\n",
    "  device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAdaptor(nn.Module) :\n",
    "  def __init__(self, vis_token_embedding_size, word_embedding_size) :\n",
    "    super(MyAdaptor, self).__init__()\n",
    "    self.vis_token_embedding_size = vis_token_embedding_size\n",
    "    self.word_embedding_size = word_embedding_size\n",
    "\n",
    "    self.adapter_linear = nn.Linear(self.vis_token_embedding_size, self.word_embedding_size)\n",
    "\n",
    "  def forward(self, img_output) :\n",
    "    self.adapter_linear.to(img_output.device)\n",
    "    img_embed = self.adapter_linear(img_output)\n",
    "    return img_embed\n",
    "\n",
    "class MyModel(nn.Module) :\n",
    "  def __init__(self) :\n",
    "    super(MyModel, self).__init__()\n",
    "    self.model_language = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b-it\", torch_dtype=torch.bfloat16)\n",
    "    self.tokenizer_language = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\", padding_side= 'right')\n",
    "    self.image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\").image_processor\n",
    "    self.model_image = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    #self.word_embedding_size = 2304\n",
    "    self.word_embedding_size = 3584\n",
    "    self.num_vocab = 256000\n",
    "\n",
    "    self.trigger_str_img = \"<start_image>\"\n",
    "    self.num_vis_token_summary = 50\n",
    "    self.vis_token_embedding_size = 768\n",
    "    self.adaptor = MyAdaptor(self.vis_token_embedding_size,self.word_embedding_size )\n",
    "    self.dummy_img_token = (\" \".join([\"the\"]*self.num_vis_token_summary)).strip()\n",
    "\n",
    "  def search_trigger_idx(self, text_token, trigger_str) :\n",
    "    all_token = text_token\n",
    "    all_string_now = \"\"\n",
    "    all_token_now = []\n",
    "    dummy_start_token = None\n",
    "    for token_idx in range(len(all_token)) :\n",
    "      token_now = int(all_token[token_idx].detach().cpu().numpy())\n",
    "      all_token_now.append(token_now)\n",
    "      token_as_string = self.tokenizer_language.batch_decode([all_token_now],skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "      if trigger_str in token_as_string :\n",
    "        dummy_start_token = token_idx + 1\n",
    "        break\n",
    "    return dummy_start_token\n",
    "\n",
    "  def get_image_embed(self, image_input) :\n",
    "    img_output = self.model_image(image_input)['last_hidden_state']\n",
    "    img_embed = self.adaptor(img_output)\n",
    "\n",
    "    return img_embed\n",
    "\n",
    "\n",
    "  def replace_embedding_hook(self, image_input) :\n",
    "    image_feature = self.get_image_embed(image_input)\n",
    "    assert len(image_feature) == 1\n",
    "\n",
    "    def now_hook(model, input, output) :\n",
    "      real_input = input[0]\n",
    "      batch_size, token_len = real_input.shape\n",
    "      if(token_len > 1) :\n",
    "        assert batch_size == 1\n",
    "        dummy_start_token = self.search_trigger_idx(real_input[0], self.trigger_str_img )\n",
    "\n",
    "        temp = image_feature[0]\n",
    "        output[:,dummy_start_token:dummy_start_token+self.num_vis_token_summary] = temp\n",
    "      return output\n",
    "    return now_hook\n",
    "\n",
    "\n",
    "\n",
    "  def split_and_replace(self, now_input_tokens, replacement_embed, start_loc) :\n",
    "    num_token = len(replacement_embed)\n",
    "\n",
    "    start_embed = now_input_tokens[0:start_loc]\n",
    "    end_embed = now_input_tokens[start_loc+num_token:]\n",
    "    # デバッグ用ログ\n",
    "    #print(f\"start_embed: {start_embed.shape}, replacement_embed: {replacement_embed.shape}, end_embed: {end_embed.shape}\")\n",
    "\n",
    "    # 必要ならサイズを調整\n",
    "    if start_embed.size(-1) != replacement_embed.size(-1) or end_embed.size(-1) != replacement_embed.size(-1):\n",
    "        print(f\"Size mismatch: start={start_embed.size(-1)}, replacement={replacement_embed.size(-1)}, end={end_embed.size(-1)}\")\n",
    "    replaced_embed = torch.cat((start_embed, replacement_embed.to(now_input_tokens.dtype), end_embed),0)\n",
    "\n",
    "    return replaced_embed\n",
    "\n",
    "  def forward_loss(self, image_input_raw, caption_output_raw) :\n",
    "    instruction_now =  \"<start_of_turn>user\\n\"\n",
    "    instruction_now += f\"<start_image> {self.dummy_img_token}\\n<end_image>\\n\"\n",
    "    instruction_now += f\"Create a simple description of the image!\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "    image_input = self.image_processor(image_input_raw, return_tensors=\"pt\")['pixel_values']\n",
    "    image_input = image_input.to(device)\n",
    "\n",
    "    caption_output = self.tokenizer_language(caption_output_raw,padding=True,return_tensors=\"pt\")\n",
    "    caption_output['input_ids'] = caption_output['input_ids'].to(device)\n",
    "    caption_output['attention_mask'] = caption_output['attention_mask'].to(device)\n",
    "\n",
    "    img_output = self.model_image(image_input)['last_hidden_state']\n",
    "    img_embed = self.adaptor(img_output)\n",
    "\n",
    "    all_text_with_prompt = [instruction_now + temp_text for temp_text in self.tokenizer_language.batch_decode(caption_output['input_ids'], skip_special_tokens=True)]\n",
    "    all_tokens_with_prompt = self.tokenizer_language(all_text_with_prompt, padding=True, return_tensors=\"pt\")\n",
    "    all_tokens_with_prompt['input_ids'] = all_tokens_with_prompt['input_ids'].to(device).detach()\n",
    "    all_tokens_with_prompt['attention_mask'] = all_tokens_with_prompt['attention_mask'].to(device).detach()\n",
    "\n",
    "    all_token_prompt_embed = self.model_language.model.embed_tokens(all_tokens_with_prompt['input_ids'])\n",
    "    prompt_len = len(self.tokenizer_language([instruction_now])['input_ids'][0])\n",
    "    caption_label_now = all_tokens_with_prompt['input_ids'][:,prompt_len:]\n",
    "    caption_label_now = F.one_hot(caption_label_now,self.num_vocab)\n",
    "    attn_mask_now = all_tokens_with_prompt['attention_mask'][:,prompt_len:]\n",
    "\n",
    "    all_replaced_feature = []\n",
    "    for temp_idx in range(len(all_tokens_with_prompt['input_ids'])) :\n",
    "      tokens_text_now = all_tokens_with_prompt['input_ids'][temp_idx].detach().cpu()\n",
    "      dummy_location_caption = self.search_trigger_idx(tokens_text_now, self.trigger_str_img )\n",
    "      image_replaced_prompt = self.split_and_replace(all_token_prompt_embed[temp_idx], img_embed[temp_idx], dummy_location_caption)\n",
    "\n",
    "      all_replaced_feature.append(image_replaced_prompt)\n",
    "    all_replaced_feature = torch.stack(all_replaced_feature)\n",
    "\n",
    "\n",
    "    logits_now = self.model_language(inputs_embeds =all_replaced_feature, attention_mask=all_tokens_with_prompt['attention_mask'])\n",
    "\n",
    "    logits_now = logits_now['logits']\n",
    "    caption_prediction_now = logits_now[:,prompt_len-1:-1]\n",
    "    caption_prediction_now = torch.softmax(caption_prediction_now,-1)\n",
    "    caption_prediction_now = torch.maximum(caption_prediction_now,torch.as_tensor(1e-10).to(caption_prediction_now.dtype))\n",
    "    caption_prediction_now = torch.minimum(caption_prediction_now,torch.as_tensor(1 - 1e-10).to(caption_prediction_now.dtype))\n",
    "\n",
    "\n",
    "    loss_lm = -torch.sum(caption_label_now*torch.log(caption_prediction_now),-1)\n",
    "    loss_lm = torch.sum(loss_lm*attn_mask_now,-1)/torch.sum(attn_mask_now,-1)\n",
    "    loss_lm = torch.mean(loss_lm)\n",
    "\n",
    "    return loss_lm\n",
    "\n",
    "  def generate_aswer_image(self, input_string, pil_image, max_new_tokens = 32, do_sample=True, top_k=50, top_p=0.95, temperature =1 ) :\n",
    "\n",
    "    input_with_dummy_prompt = self.tokenizer_language.apply_chat_template(input_string, tokenize=False, add_generation_prompt=True)\n",
    "    input_with_dummy_prompt = input_with_dummy_prompt.replace(\"<image>\", \"<start_image> \"+self.dummy_img_token+\"\\n<end_image>\")\n",
    "    dummy_input = self.tokenizer_language(input_with_dummy_prompt,padding=True,return_tensors=\"pt\")\n",
    "    dummy_input['input_ids'] = dummy_input['input_ids'].to(device)\n",
    "    dummy_input['attention_mask'] = dummy_input['attention_mask'].to(device)\n",
    "    assert len(dummy_input['input_ids']) == 1\n",
    "\n",
    "    handler_image = None\n",
    "\n",
    "    contains_image = False\n",
    "    if self.trigger_str_img in input_with_dummy_prompt :\n",
    "      image_input = self.image_processor([pil_image], return_tensors=\"pt\")['pixel_values'].to(device)\n",
    "      hook_now_image = self.replace_embedding_hook(image_input)\n",
    "      contains_image = True\n",
    "      handler_image = self.model_language.model.embed_tokens.register_forward_hook(hook_now_image)\n",
    "\n",
    "\n",
    "\n",
    "    output_now = self.model_language.generate(**dummy_input,\n",
    "                                              max_new_tokens = max_new_tokens,\n",
    "                                              do_sample=do_sample,\n",
    "                                              temperature=temperature,\n",
    "                                              top_k=top_k,\n",
    "                                              top_p=top_p,\n",
    "                                              )\n",
    "    output_string = self.tokenizer_language.batch_decode(output_now, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    if contains_image :\n",
    "      handler_image.remove()\n",
    "\n",
    "    return output_string.split(\"model\\n\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.83s/it]\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lsemi\\AppData\\Local\\Temp\\ipykernel_1644\\3914503102.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.adaptor.load_state_dict(torch.load(SAVED_PATH), strict=False)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Lsemi\\anaconda3\\envs\\langchain\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n",
      "File \u001b[1;32mc:\\Users\\Lsemi\\anaconda3\\envs\\langchain\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.71 GiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 11.31 GiB is allocated by PyTorch, and 1.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39madaptor\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(SAVED_PATH), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lsemi\\anaconda3\\envs\\langchain\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lsemi\\anaconda3\\envs\\langchain\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lsemi\\anaconda3\\envs\\langchain\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lsemi\\anaconda3\\envs\\langchain\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lsemi\\anaconda3\\envs\\langchain\\Lib\\site-packages\\torch\\nn\\modules\\module.py:926\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam_applied\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n",
      "File \u001b[1;32mc:\\Users\\Lsemi\\anaconda3\\envs\\langchain\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:84\u001b[0m, in \u001b[0;36mno_grad.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[0;32m     82\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     85\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model.to(torch.bfloat16)\n",
    "model.adaptor.load_state_dict(torch.load(SAVED_PATH), strict=False)\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
